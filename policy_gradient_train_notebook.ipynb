{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["**It is recommended to run this notebook on GPU.**"],"metadata":{"id":"wyO1QQfitc2p"}},{"cell_type":"markdown","source":["# Initializations"],"metadata":{"id":"Ca6KqETCMvo2"}},{"cell_type":"markdown","source":["Instructions:\n","\n","1. Put the files in your google drive.\n","2. Write down the drive project folder path in the **`PATH`** variable.\n","3. Run the cells to train the model"],"metadata":{"id":"QFQgU5IQ3wE3"}},{"cell_type":"code","source":["PATH = \"PATH_TO_Blockudoku-ai_FOLDER\" # the path \"Blockudoku-ai\" folder in google drive"],"metadata":{"id":"z3jPReLbM2Rl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YDZ1fTJvhPD3","executionInfo":{"status":"ok","timestamp":1726208055064,"user_tz":-180,"elapsed":24666,"user":{"displayName":"Israel Ben David","userId":"03645948270278159168"}},"outputId":"3d056019-ec35-41b4-deb5-3f9209d5a7ba"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import sys\n","sys.path.append(PATH)\n","import importlib\n","import PolicyGradientAgent\n","import Engine\n","importlib.reload(PolicyGradientAgent)\n","importlib.reload(Engine)\n","csv_path = None"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"POg3Q_U3hSer","executionInfo":{"status":"ok","timestamp":1726208064193,"user_tz":-180,"elapsed":9133,"user":{"displayName":"Israel Ben David","userId":"03645948270278159168"}},"outputId":"68c3b221-7404-46c5-c9a6-63b494880da6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["pygame 2.6.0 (SDL 2.28.4, Python 3.10.12)\n","Hello from the pygame community. https://www.pygame.org/contribute.html\n"]}]},{"cell_type":"markdown","source":["# Pretraining Uniform Policy Network\n","In this section you can pretrain the policy network to learn the valid actions for each state.\n","The training objective is reducing the KL-divergence between the output of the network and the uniform distribution accross all valid actions in each state."],"metadata":{"id":"tPjkFO0m41j-"}},{"cell_type":"code","source":["import importlib\n","import PolicyGradientUniform\n","import Engine\n","importlib.reload(PolicyGradientUniform)\n","importlib.reload(Engine)"],"metadata":{"id":"hZhBVMHn5JTA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["PRETRAIN_PATH = f\"{PATH}/checkpoints/pg_unif/pg_unif_.pth\"\n","game = Engine.Blockudoku()\n","agent = PolicyGradientUniform.PGUniformAgent(game)\n","agent.train(100000000, 100, save=True, save_path=PRETRAIN_PATH, lr=0.00001)"],"metadata":{"id":"ehBe4MaN4MSO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Training Policy Gradient Agent"],"metadata":{"id":"7NEoCcXz5DgF"}},{"cell_type":"markdown","source":["Optional - set csv to hold the training records"],"metadata":{"id":"RytEIH6P8SSb"}},{"cell_type":"code","source":["import csv\n","csv_path = f\"{PATH}/checkpoints/records/new_train.csv\"\n","\n","data = [\"batch\", \"steps\", \"invalids\", \"reward\", \"score\"]\n","# Open the file in append mode ('a') and write the data\n","with open(csv_path, 'w', newline='') as file:\n","  writer = csv.writer(file)\n","  writer.writerow(data)  # Write a single row to the CSV file"],"metadata":{"id":"Ane3tABi5ivG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Load the agent"],"metadata":{"id":"x-uwqNxD-2XT"}},{"cell_type":"code","source":["game = Engine.Blockudoku()\n","agent = PolicyGradientAgent.PolicyGradientAgent(game)\n","# load the pretrained model\n","agent.load_model(PRETRAIN_PATH) # comment this line if you wish to train the model directly without the pretraining phase"],"metadata":{"id":"x_Y9hi87-wVB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Option 1: Train the agent using gradualy discount factor increment strategy"],"metadata":{"id":"YAcMyzF95n-z"}},{"cell_type":"code","source":["gamma_stepsize = 0.1\n","agent.set_epsilon(0.2)\n","\n","for lesson in range(5, 20):\n","  discount_factor = gamma_stepsize * lesson\n","  if discount_factor >= 0.99: break\n","  print(f\"Training with discount factor: {discount_factor}\")\n","  agent.set_gamma(discount_factor)\n","  # agent.train(20001, 100, render=False, save=True,\n","  agent.train(50001, 100, render=False, save=True,\n","              save_path=f\"{PATH}/checkpoints/new_train/pg_model_lesson_{lesson}.pth\",\n","              lr=0.00001,\n","              csv_path=csv_path)"],"metadata":{"id":"HEIzWOINir5s"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Option 2: Train the agent using constant discount factor."],"metadata":{"id":"7cT7rkHt8zmr"}},{"cell_type":"code","source":["agent.set_epsilon(0.2)\n","agent.set_gamma(0.85)\n","\n","agent.train(10000000, 100,\n","            render=False, save=True,\n","            save_path=f\"{PATH}/checkpoints/new_train/pg_model_.pth\",\n","            lr=0.00001,\n","            csv_path=csv_path)"],"metadata":{"id":"QAXWmaYahaG1"},"execution_count":null,"outputs":[]}]}